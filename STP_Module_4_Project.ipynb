{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Abhiram-kopalle/AIML_Projects_and_Labs/blob/main/STP_Module_4_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Module 4: Linear Classifiers & Gradient Descent"
      ],
      "metadata": {
        "id": "xET1TacFQWQ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Case Study: Predictive Modeling for Public Water Safety**\n",
        "\n",
        "**Objective:** Develop a robust classifier to identify potable water samples. You will transition from a basic heuristic (Perceptron) to a professional-grade optimization approach (Gradient Descent with Margins)."
      ],
      "metadata": {
        "id": "_V7gSkZHQXdf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Data Acquisition & Cleaning\n",
        "\n",
        "In real-world data science, datasets are rarely perfect. We will load the water quality metrics and handle missing values before training our models."
      ],
      "metadata": {
        "id": "YjRDChliR1BK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load the dataset from a public raw GitHub URL\n",
        "url = \"https://raw.githubusercontent.com/nferran/tp_aprendizaje_de_maquina_I/main/water_potability.csv\"\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "# Step 1: Handling Missing Values\n",
        "# Water sensors often fail, leaving NaNs. We will fill them with the mean of the column.\n",
        "df.fillna(df.mean(), inplace=True)\n",
        "\n",
        "# Step 2: Feature Selection & Labeling\n",
        "# We'll use all chemical features to predict 'Potability'\n",
        "X = df.drop('Potability', axis=1).values\n",
        "y = df['Potability'].values\n",
        "\n",
        "# Step 3: Class Label Conversion\n",
        "# Many linear classifiers (like Perceptron/SVM) require labels to be -1 and 1\n",
        "y = np.where(y == 0, -1, 1)\n",
        "\n",
        "# Step 4: Train-Test Split & Scaling\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "print(f\"Dataset Loaded: {X_train.shape[0]} training samples, {X_train.shape[1]} features.\")"
      ],
      "metadata": {
        "id": "LOg9j_w7R8uU",
        "outputId": "da067c1f-1c62-438d-9b87-51deeaf0ae6f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset Loaded: 2620 training samples, 9 features.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Phase 1: The Heuristic Approach (Perceptron)\n",
        "\n",
        "The **Perceptron** represents the earliest form of supervised learning. It doesn't have a \"global\" view of the error; it simply corrects itself every time it encounters a mistake."
      ],
      "metadata": {
        "id": "_Mr2gopTSBjh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task:** Implement the Perceptron Update Rule inside the training loop."
      ],
      "metadata": {
        "id": "h16OQo9LSLDP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class WaterPerceptron:\n",
        "    def __init__(self, lr=0.01, epochs=50):\n",
        "        self.lr = lr\n",
        "        self.epochs = epochs\n",
        "        self.w = None\n",
        "        self.b = 0\n",
        "        self.mistakes = []\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.w = np.zeros(X.shape[1])\n",
        "        self.b = 0\n",
        "\n",
        "        for epoch in range(self.epochs):\n",
        "            count = 0\n",
        "\n",
        "            for i in range(len(y)):\n",
        "                linear_output = np.dot(X[i], self.w) + self.b\n",
        "                prediction = np.sign(linear_output)\n",
        "\n",
        "                if prediction == 0:\n",
        "                    prediction = 1\n",
        "\n",
        "                if y[i] * prediction <= 0:\n",
        "                    self.w = self.w + self.lr * y[i] * X[i]\n",
        "                    self.b = self.b + self.lr * y[i]\n",
        "                    count += 1\n",
        "\n",
        "            self.mistakes.append(count)\n",
        "\n",
        "    def predict(self, X):\n",
        "        return np.sign(np.dot(X, self.w) + self.b)\n"
      ],
      "metadata": {
        "id": "mqxF5SQGSA8-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Phase 2: Gradient Descent - Global Optimization\n",
        "\n",
        "The Perceptron is unstable if the data isn't perfectly separable. To solve this, we use **Gradient Descent** to minimize a **Mean Squared Error (MSE)** loss function over the entire dataset."
      ],
      "metadata": {
        "id": "Xzzq_ziOSQmV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task:** Implement the batch gradient calculation for weights and bias."
      ],
      "metadata": {
        "id": "xJzJuR77SRiM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GDWaterClassifier:\n",
        "    def __init__(self, lr=0.001, epochs=500):\n",
        "        self.lr = lr\n",
        "        self.epochs = epochs\n",
        "        self.w = None\n",
        "        self.b = 0\n",
        "        self.cost_history = []\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.w = np.zeros(X.shape[1])\n",
        "        self.b = 0\n",
        "        n = X.shape[0]\n",
        "\n",
        "        for _ in range(self.epochs):\n",
        "            z = np.dot(X, self.w) + self.b\n",
        "\n",
        "            dw = (1/n) * np.dot(X.T, (z - y))\n",
        "            db = (1/n) * np.sum(z - y)\n",
        "\n",
        "            self.w = self.w - self.lr * dw\n",
        "            self.b = self.b - self.lr * db\n",
        "\n",
        "            cost = (1/(2*n)) * np.sum((z - y) ** 2)\n",
        "            self.cost_history.append(cost)\n",
        "\n",
        "    def predict(self, X):\n",
        "        return np.sign(np.dot(X, self.w) + self.b)\n"
      ],
      "metadata": {
        "id": "Q9iFCaG3Se2L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Phase 3: Margin Classifiers & Hinge Loss\n",
        "\n",
        "In water safety, we aim for more than just correctness—we want a **Margin**, a safety gap between safe and unsafe samples. This is achieved using **Hinge Loss** combined with **L2 Regularization**.\n",
        "\n",
        "The loss function is defined as:\n",
        "\n",
        "$$\n",
        "\\text{Loss} = \\lambda \\|w\\|^2_2 + \\sum_{i} \\max(0, 1 - y_i (w^T x_i + b))\n",
        "$$\n",
        "\n",
        "### Key Components:\n",
        "- **Hinge Loss**: $\\max(0, 1 - y_i (w^T x_i + b))$ ensures correct classification with a margin.\n",
        "- **L2 Regularization**: $\\lambda \\|w\\|^2_2$ penalizes large weights, promoting generalization and stability.\n"
      ],
      "metadata": {
        "id": "xT9CDlzUSf65"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MarginWaterClassifier:\n",
        "    def __init__(self, lr=0.001, lambda_param=0.01, epochs=500):\n",
        "        self.lr = lr\n",
        "        self.lambda_param = lambda_param\n",
        "        self.epochs = epochs\n",
        "        self.w = None\n",
        "        self.b = 0\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.w = np.zeros(X.shape[1])\n",
        "        for _ in range(self.epochs):\n",
        "            for i, x_i in enumerate(X):\n",
        "                # TODO: Implement the Margin Condition check: y_i * (w * x_i + b) >= 1\n",
        "                if False: # Replace False with condition\n",
        "                    # Only Regularization update\n",
        "                    self.w -= self.lr * (2 * self.lambda_param * self.w)\n",
        "                else:\n",
        "                    # Update for weight (including Hinge Loss) and bias\n",
        "                    # self.w -= self.lr * (2 * self.lambda_param * self.w - x_i * y[i])\n",
        "                    # self.b -= self.lr * (-y[i])\n",
        "                    pass\n",
        "\n",
        "    def predict(self, X):\n",
        "        return np.sign(np.dot(X, self.w) + self.b)"
      ],
      "metadata": {
        "id": "RSLdAztpS03K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Critical Analysis & Comparison\n",
        "\n",
        "**Analysis Tasks:**\n",
        "1. Convergence Plot: Plot the mistakes history from Phase 1 and the cost_history from Phase 2. Discuss why the Gradient Descent plot is smoother.\n",
        "\n",
        "The Phase 1 mistakes plot is usually jagged because Perceptron updates only when it misclassifies and the number of mistakes can jump suddenly.\n",
        "The Phase 2 cost_history from Gradient Descent looks smoother because it updates weights with small continuous steps and steadily reduces loss.\n",
        "Gradient Descent averages information over all samples each iteration, so the curve changes gradually.\n",
        "That’s why the GD curve shows more stable convergence than mistake-count updates.\n",
        "\n",
        "2. Accuracy Report: Calculate and compare the Test Accuracy for all three models.\n",
        "\n",
        "Compute test accuracy for all three models (Perceptron, Margin classifier, Gradient Descent) on the same test set and compare them directly.\n",
        "Typically, the Margin and Gradient Descent models generalize better than plain Perceptron on noisy data.\n",
        "Perceptron may perform worse because it is sensitive to non-separable/noisy points.\n",
        "The best model is the one with highest test accuracy and stable performance.\n",
        "\n",
        "3. Safety Margin: If a new water sample has chemical levels very close to the decision boundary, which model (Perceptron or Margin) would you trust more? Why?\n",
        "\n",
        "I would trust the Margin model more for samples close to the decision boundary.\n",
        "Because it maximizes the margin, it creates a safer buffer zone between classes.\n",
        "Perceptron only finds any separating line and can be less stable near the boundary.\n",
        "So margin-based decisions are more reliable when inputs are borderline"
      ],
      "metadata": {
        "id": "VOxVmUrBS64Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Discussion Questions\n",
        "\n",
        "### Q1: Impact of High Learning Rate in Gradient Descent\n",
        "What happens to your **Gradient Descent** model if you set the `learning_rate` too high (e.g., `1.0`)?\n",
        "*Hint: Think about convergence, overshooting, and divergence.*\n",
        "\n",
        "If the learning rate is too high (like 1.0), gradient descent takes very large steps and overshoots the minimum.\n",
        "Instead of smoothly decreasing, the loss may oscillate up and down.\n",
        "In many cases it fails to converge and starts diverging.\n",
        "So the model becomes unstable and training does not learn properly.\n",
        "---\n",
        "\n",
        "### Q2: Label Conversion in Classification\n",
        "Why did we convert the labels to **$\\{-1, 1\\}$** instead of keeping them as **$\\{0, 1\\}$**?\n",
        "*Hint: Consider the mathematical formulation of the loss function (e.g., Hinge Loss) and symmetry.*\n",
        "\n",
        "We convert labels to {-1, 1} because many losses like hinge loss use the form max(0, 1 − y·(wᵀx+b)).\n",
        "This formulation naturally assumes y is either +1 or −1 for symmetry around the decision boundary.\n",
        "With {0,1}, the term y·(wᵀx+b) breaks for class 0 and the margin idea doesn’t work cleanly.\n",
        "So {-1,1} makes the math and updates simpler and more correct.\n",
        "---\n",
        "\n",
        "### Q3: Handling Noisy Data (Water Potability Dataset)\n",
        "The **Water Potability dataset** is often \"noisy\" (not perfectly separable). Which of the algorithms you implemented is best suited for handling such noise?\n",
        "*Hint: Think about robustness to outliers and margin-based classifiers.*\n",
        "\n",
        "For noisy datasets like Water Potability, soft-margin SVM (regularized margin-based classifier) performs best.\n",
        "It allows some misclassifications while still maximizing the margin, so it doesn’t overreact to noise.\n",
        "Perceptron can struggle because it expects separable data and may not converge.\n",
        "Thus regularized margin-based methods are more robust for noisy and imperfect data.\n"
      ],
      "metadata": {
        "id": "jgYdUvAvTDxy"
      }
    }
  ]
}